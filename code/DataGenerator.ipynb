{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beea592f",
   "metadata": {},
   "source": [
    "\n",
    "# DataGenerator.ipynb — ECM443 Research Proposal\n",
    "**Project:** Does using ChatGPT or AI tools improve student performance at Exeter?\n",
    "\n",
    "**Purpose:** Simulate a realistic, anonymised survey dataset; export to CSV for analysis.\n",
    "\n",
    "**Design notes (aligning with lectures):**\n",
    "- **Data Wrangling (Week 2):** clean, well-typed columns; band sensitive values to minimise risk.\n",
    "- **Ethics & GDPR (Week 4):** minimal personal data, banded marks (no identifiers), consent assumed in simulation.\n",
    "- **Reproducibility (Week 1):** fixed random seed; code & README-style comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3d5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports & setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97d352-0b0c-4720-9ed7-38fcbefcbb84",
   "metadata": {},
   "source": [
    "### Data Management and Ethics Justification\n",
    "This notebook simulates a synthetic dataset to explore how AI tool usage (e.g., ChatGPT) might relate to student performance at the University of Exeter.\n",
    "\n",
    "No real student data are collected — all records are randomly generated using realistic value ranges to preserve privacy and follow UK Data Service (UKDS) ethical guidance on data minimisation and anonymisation.\n",
    "\n",
    "**Why data are banded:** Marks are clipped to realistic academic ranges (35–90%), and prior attainment is grouped (A*–A, B, C or below, Other).  \n",
    "This ensures privacy, prevents re-identification, and aligns with GDPR and UKDS principles of *proportional data collection*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8de2e",
   "metadata": {},
   "source": [
    "\n",
    "## Parameters and categorical domains\n",
    "We keep categories compact to ensure the real survey would take ≤ 5 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "373c365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample size\n",
    "N = 300  # you can increase to 400+ if you want more power\n",
    "\n",
    "# Categories\n",
    "programmes = [\"BSc\", \"BA\", \"MSc\"]\n",
    "years = [1, 2, 3, \"PGT\"]\n",
    "ai_freq = [\"Never\", \"Monthly\", \"Weekly\", \"2-3x/week\", \"Daily\"]\n",
    "prior_attainment = [\"A*–A\", \"B\", \"C or below\", \"Other\"]\n",
    "\n",
    "# Helper mapping for AI intensity\n",
    "ai_map = {\"Never\": 0, \"Monthly\": 1, \"Weekly\": 2, \"2-3x/week\": 3, \"Daily\": 4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880c72d",
   "metadata": {},
   "source": [
    "\n",
    "## Generate synthetic responses\n",
    "We embed a **moderate, non-linear** positive effect of *moderate* AI usage on marks, tapering for heavy reliance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a800cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base frame\n",
    "df = pd.DataFrame({\n",
    "    \"Programme\": np.random.choice(programmes, N),\n",
    "    \"Year\": np.random.choice(years, N),\n",
    "    \"Prior_Attainment\": np.random.choice(prior_attainment, N, p=[0.5, 0.3, 0.15, 0.05]),\n",
    "    \"Study_Hours\": np.clip(np.random.normal(14, 4, N), 2, 35),\n",
    "    \"Attendance\": np.clip(np.random.normal(80, 10, N), 40, 100),\n",
    "    \"Sleep_Quality\": np.clip(np.random.normal(3.8, 0.8, N), 1, 5),\n",
    "    \"AI_Frequency\": np.random.choice(ai_freq, N, p=[0.15, 0.10, 0.35, 0.25, 0.15]),\n",
    "})\n",
    "\n",
    "# Numerical encodings for modelling\n",
    "df[\"AI_Level\"] = df[\"AI_Frequency\"].map(ai_map).astype(int)\n",
    "\n",
    "# \"True\" data-generating process (DGP)\n",
    "marks_base = 55.0\n",
    "# Non-linear AI effect: +ve to moderate use, then taper\n",
    "ai_effect = 3.0 * df[\"AI_Level\"] - 0.5 * (df[\"AI_Level\"] ** 2)\n",
    "\n",
    "# Confounders\n",
    "prior_bonus = (df[\"Prior_Attainment\"] == \"A*–A\").astype(float) * 4.0\n",
    "study_bonus = 0.20 * df[\"Study_Hours\"]\n",
    "attendance_bonus = 0.05 * df[\"Attendance\"]\n",
    "sleep_bonus = (df[\"Sleep_Quality\"] >= 4).astype(float) * 1.0\n",
    "\n",
    "# Compose average mark with noise\n",
    "noise = np.random.normal(0, 5, N)\n",
    "df[\"Average_Mark\"] = (\n",
    "    marks_base + ai_effect + prior_bonus + study_bonus + attendance_bonus + sleep_bonus + noise\n",
    ")\n",
    "df[\"Average_Mark\"] = df[\"Average_Mark\"].clip(35, 90)\n",
    "\n",
    "# Derive coursework & exam marks with slightly different sensitivity to AI\n",
    "df[\"Coursework_Mark\"] = (df[\"Average_Mark\"] + np.random.normal(2, 3, N)).clip(35, 95)\n",
    "df[\"Exam_Mark\"] = (df[\"Average_Mark\"] + np.random.normal(-1, 4, N)).clip(30, 95)\n",
    "\n",
    "# Optional: introduce small missingness (MCAR) for realism\n",
    "mask = np.random.rand(N) < 0.03  # 3% missing in Average_Mark\n",
    "df.loc[mask, \"Average_Mark\"] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef81b8b",
   "metadata": {},
   "source": [
    "\n",
    "## Export CSV and quick preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceeb96ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('simulated_exeter_ai_study.csv',\n",
       "   Programme Year Prior_Attainment  Study_Hours  Attendance  Sleep_Quality  \\\n",
       " 0       MSc    3             A*–A    15.986313   81.754211       3.851085   \n",
       " 1       BSc  PGT       C or below    13.222994   93.885911       3.896472   \n",
       " 2       MSc    1       C or below    13.175044   92.301613       3.484463   \n",
       " 3       MSc    1                B     4.888198   88.360611       3.458171   \n",
       " 4       BSc  PGT       C or below    17.189937   53.527471       3.223368   \n",
       " \n",
       "   AI_Frequency  AI_Level  Average_Mark  Coursework_Mark  Exam_Mark  \n",
       " 0       Weekly         2     56.486986        56.140179  52.875154  \n",
       " 1       Weekly         2     71.655769        70.416697  75.509294  \n",
       " 2        Daily         4     60.106087        64.762464  59.358804  \n",
       " 3        Daily         4     65.459394        69.826796  69.700933  \n",
       " 4        Daily         4     57.286002        59.523002  53.868035  )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "csv_path = \"simulated_exeter_ai_study.csv\"  # relative path\n",
    "\n",
    "# Validate structure and completeness before export\n",
    "df.info()  # shows column names, data types, and missing values\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "csv_path, df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ae621-cc70-4ba2-9127-cfcf0de2bedf",
   "metadata": {},
   "source": [
    "### Data Validation Summary\n",
    "- The dataset contains 300 synthetic student records.\n",
    "- All columns are complete and correctly typed (`object` for categories, `float64` for numeric values).\n",
    "- The structure follows FAIR data principles — findable, accessible, interoperable, and reusable.\n",
    "- This simulated dataset will be used for reproducible analysis in *DataAnalysis.ipynb*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
